{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define data loader (example data)\n",
    "# Assuming you already have a DataLoader for loading COCO Caption data\n",
    "\n",
    "# Initialize model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Stochastic EM iterations\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        image_inputs = batch['image'].to(device)  # Image data\n",
    "        text_inputs = batch['text']  # Text descriptions\n",
    "        \n",
    "        # Step 3: Calculate similarity scores (computed using CLIP here)\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        similarity_scores = torch.matmul(image_features, text_features.t())\n",
    "\n",
    "        # Step 5: Simulate auxiliary random variables and weights (needs to be implemented based on model-specific requirements)\n",
    "        u = torch.rand(image_inputs.shape[0], device=device)  # Example random auxiliary variable\n",
    "        w_plus = torch.rand(image_inputs.shape[0], device=device)  # Example positive pair weights\n",
    "        w_minus = torch.rand(image_inputs.shape[0], image_inputs.shape[0], device=device)  # Example negative pair weights\n",
    "\n",
    "        # Step 6: Calculate weighted contrastive loss\n",
    "        contrastive_loss = -torch.log(F.softmax(similarity_scores, dim=1)[:, 0])  # Consider only the first column as positive\n",
    "        \n",
    "        weighted_contrastive_loss = (w_plus * similarity_scores[:, 0] / \n",
    "                                     (w_plus * similarity_scores[:, 0] + torch.sum(w_minus * similarity_scores, dim=1)))\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = torch.mean(weighted_contrastive_loss)\n",
    "\n",
    "        # Step 10: Update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# After training, model parameters have been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =256\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco2014/images', text_path='/home/jason/data/coco2014/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco2014/images','/home/jason/data/coco2014/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info NCE loss \n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load the COCO caption dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "coco_dataset = CocoCaptions(root='path_to_coco_dataset', annFile='path_to_annotations_file', transform=transform)\n",
    "\n",
    "# Create a data loader\n",
    "data_loader = DataLoader(coco_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "temperature = 0.07  # InfoNCE temperature parameter\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, captions in data_loader:\n",
    "        # Image preprocessing\n",
    "        image_inputs = processor(images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        # Text encoding\n",
    "        text_inputs = processor(captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Compute image and text features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**image_inputs)\n",
    "            text_features = model.get_text_features(**text_inputs)\n",
    "\n",
    "        # Calculate positive sample similarity\n",
    "        sim_pos = (text_features @ image_features.T) / temperature\n",
    "\n",
    "        # Randomly select a negative sample\n",
    "        indices = torch.randperm(len(images)).to(device)\n",
    "        neg_captions = captions[indices]\n",
    "\n",
    "        # Calculate negative sample similarity\n",
    "        text_inputs_neg = processor(neg_captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        text_features_neg = model.get_text_features(**text_inputs_neg)\n",
    "        sim_neg = (text_features_neg @ image_features.T) / temperature\n",
    "\n",
    "        # Calculate InfoNCE loss\n",
    "        logits = torch.cat([sim_pos, sim_neg], dim=1)\n",
    "        labels = torch.zeros(len(images), dtype=torch.long).to(device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"path_to_save_model\")\n",
    "processor.save_pretrained(\"path_to_save_processor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
