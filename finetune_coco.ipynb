{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b362038be140c6b1ddb0730479eea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_915367/872034600.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),torch.tensor(U*s_plus + b_plus).float())\n",
      "/tmp/ipykernel_915367/872034600.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),torch.tensor(U.unsqueeze(1)*s_minus + b_minus).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 16 and 15 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iters):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     U \u001b[39m=\u001b[39m sample_u(weights, logits_per_image)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     weights \u001b[39m=\u001b[39m sample_w(U, logits_per_image)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m weighted_sim \u001b[39m=\u001b[39m weights \u001b[39m*\u001b[39m logits_per_image\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss_fn(weighted_sim, torch\u001b[39m.\u001b[39marange(BATCH_SIZE)\u001b[39m.\u001b[39mto(device))\n",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m w_plus \u001b[39m=\u001b[39m w_plus_dist\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m w_minus \u001b[39m=\u001b[39m w_minus_dist\u001b[39m.\u001b[39msample()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((w_plus[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), w_minus\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m w_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((tmp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), w_plus[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\u001b[39m.\u001b[39mview(BS, \u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m w_matrix\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 16 and 15 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =16\n",
    "# Parameters\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "iters = 2\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco2014/images', text_path='/home/jason/data/coco2014/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco2014/images','/home/jason/data/coco2014/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool())\n",
    "    s_minus = s_matrix.masked_select(torch.eye(BS).bool())\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),torch.tensor(U*s_plus + b_plus).float())\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),torch.tensor(U.unsqueeze(1)*s_minus + b_minus).float())\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1), w_minus.transpose(1, 0)), dim=1)\n",
    "    w_matrix = torch.cat((tmp.view(-1), w_plus[-1].unsqueeze(0))).view(BS, 4)\n",
    "    return w_matrix\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device), rate_param.float())\n",
    "    return u_dist.sample()\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        # ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        # total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        weights = torch.ones(BATCH_SIZE).to(device)\n",
    "        for _ in range(iters):\n",
    "            U = sample_u(weights, logits_per_image)\n",
    "            weights = sample_w(U, logits_per_image)\n",
    "\n",
    "        weighted_sim = weights * logits_per_image\n",
    "        total_loss = loss_fn(weighted_sim, torch.arange(BATCH_SIZE).to(device))\n",
    "\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Implementation about Cross-Entropy\n",
    "\n",
    "\n",
    "a = torch.rand(3, 3)\n",
    "print(a)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "b = torch.arange(3)\n",
    "print(b)\n",
    "l1 = loss(a, b)\n",
    "print(l1)\n",
    "\n",
    "c = -torch.log(F.softmax(a, dim=1).masked_select(torch.eye(a.size(0), a.size(1), dtype=torch.bool)))\n",
    "\n",
    "# print(-torch.log(F.softmax(a, dim=1)))\n",
    "# print(c)\n",
    "\n",
    "# diagonal_mask = torch.eye(matrix.size(0), matrix.size(1), dtype=torch.bool)\n",
    "# d = matrix.masked_select(diagonal_mask)\n",
    "print(c.mean())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
