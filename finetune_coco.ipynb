{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b362038be140c6b1ddb0730479eea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_915367/872034600.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),torch.tensor(U*s_plus + b_plus).float())\n",
      "/tmp/ipykernel_915367/872034600.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),torch.tensor(U.unsqueeze(1)*s_minus + b_minus).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 16 and 15 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iters):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     U \u001b[39m=\u001b[39m sample_u(weights, logits_per_image)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     weights \u001b[39m=\u001b[39m sample_w(U, logits_per_image)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m weighted_sim \u001b[39m=\u001b[39m weights \u001b[39m*\u001b[39m logits_per_image\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss_fn(weighted_sim, torch\u001b[39m.\u001b[39marange(BATCH_SIZE)\u001b[39m.\u001b[39mto(device))\n",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m w_plus \u001b[39m=\u001b[39m w_plus_dist\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m w_minus \u001b[39m=\u001b[39m w_minus_dist\u001b[39m.\u001b[39msample()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((w_plus[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), w_minus\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m w_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((tmp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), w_plus[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\u001b[39m.\u001b[39mview(BS, \u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m w_matrix\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 16 and 15 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "EPOCH =10\n",
    "BATCH_SIZE =16\n",
    "# Parameters\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "iters = 2\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco2014/images', text_path='/home/jason/data/coco2014/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco2014/images','/home/jason/data/coco2014/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool())\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool())\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),torch.tensor(U*s_plus + b_plus).float())\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),torch.tensor(U.unsqueeze(1)*s_minus + b_minus).float())\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1), w_minus.transpose(1, 0)), dim=1)\n",
    "    w_matrix = torch.cat((tmp.view(-1), w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    return w_matrix\n",
    "\n",
    "    \n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device), rate_param.float())\n",
    "    return u_dist.sample()\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "      \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        # ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        # total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        weights = torch.ones(BATCH_SIZE).to(device)\n",
    "        for _ in range(iters):\n",
    "            U = sample_u(weights, logits_per_image)\n",
    "            weights = sample_w(U, logits_per_image)\n",
    "\n",
    "        weighted_sim = weights * logits_per_image\n",
    "        total_loss = loss_fn(weighted_sim, torch.arange(BATCH_SIZE).to(device))\n",
    "\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Implementation about Cross-Entropy\n",
    "\n",
    "\n",
    "a = torch.rand(3, 3)\n",
    "print(a)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "b = torch.arange(3)\n",
    "print(b)\n",
    "l1 = loss(a, b)\n",
    "print(l1)\n",
    "\n",
    "c = -torch.log(F.softmax(a, dim=1).masked_select(torch.eye(a.size(0), a.size(1), dtype=torch.bool)))\n",
    "\n",
    "# print(-torch.log(F.softmax(a, dim=1)))\n",
    "# print(c)\n",
    "\n",
    "# diagonal_mask = torch.eye(matrix.size(0), matrix.size(1), dtype=torch.bool)\n",
    "# d = matrix.masked_select(diagonal_mask)\n",
    "print(c.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_param: tensor([2.7797, 1.5095, 2.2953, 1.5791], device='cuda:2')\n",
      "U: torch.Size([4])\n",
      "s_minus: torch.Size([12])\n",
      "U.unsqueeze: torch.Size([4, 12])\n",
      "w_plus: torch.Size([4])\n",
      "w_minus: torch.Size([4, 12])\n",
      "w_plus[:-1]: torch.Size([3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 12 and 3 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     U \u001b[39m=\u001b[39m sample_u(weights, similarity_score)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mU:\u001b[39m\u001b[39m\"\u001b[39m, U\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     weights \u001b[39m=\u001b[39m sample_w(U, similarity_score)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(weights)\n",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mw_minus:\u001b[39m\u001b[39m\"\u001b[39m, w_minus\u001b[39m.\u001b[39msize()) \u001b[39m#torch.Size([4, 12])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mw_plus[:-1]:\u001b[39m\u001b[39m'\u001b[39m, w_plus[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((w_plus[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), w_minus\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m w_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((tmp\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), w_plus[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\u001b[39m.\u001b[39mview(BS, \u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m w_matrix\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 12 and 3 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    print(\"rate_param:\", rate_param)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device), rate_param.float())\n",
    "    return u_dist.sample()\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool()).to(device)\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool()).to(device)\n",
    "    print(\"s_minus:\", s_minus.size()) #torch.Size([12])\n",
    "\n",
    "    w_plus_dist = Gamma(1+a_plus, (U*s_plus + b_plus))\n",
    "    w_minus_dist = Gamma(a_minus, (U.unsqueeze(1)*s_minus + b_minus))\n",
    "    print(\"U.unsqueeze:\", (U.unsqueeze(1)*s_minus).size()) #torch.Size([4, 12])\n",
    "\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    print(\"w_plus:\", w_plus.size()) #torch.Size([4])\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    print(\"w_minus:\", w_minus.size()) #torch.Size([4, 12])\n",
    "    print('w_plus[:-1]:', w_plus[:-1].size())\n",
    "\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1), w_minus.transpose(1, 0)), dim=1)\n",
    "\n",
    "    \n",
    "    w_matrix = torch.cat((tmp.view(-1), w_plus[-1].unsqueeze(0))).view(BS, 4)\n",
    "    return w_matrix\n",
    "\n",
    "BATCH_SIZE =4\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "for _ in range(2):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_param: tensor([2.5934, 2.1994, 1.6663, 1.8559], device='cuda:2')\n",
      "U: torch.Size([4])\n",
      "s_minus: torch.Size([12])\n",
      "U: tensor([0.1638, 0.0477, 5.3180, 0.5599], device='cuda:2')\n",
      "U: tensor([0.1638, 0.1638, 0.1638, 0.0477, 0.0477, 0.0477, 5.3180, 5.3180, 5.3180,\n",
      "        0.5599, 0.5599, 0.5599], device='cuda:2')\n",
      "w_plus: torch.Size([4])\n",
      "w_minus: torch.Size([12])\n",
      "w_plus[:-1]: torch.Size([3])\n",
      "tensor([[ 20.1765,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000, 115.2418,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,  26.2004,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  34.0093]], device='cuda:2')\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [ True,  True,  True, False]])\n",
      "rate_param: tensor([183.1842, 633.2631,   6.7449,  46.6484], device='cuda:2')\n",
      "U: torch.Size([4])\n",
      "s_minus: torch.Size([12])\n",
      "U: tensor([0.0007, 0.0006, 0.1155, 0.0384], device='cuda:2')\n",
      "U: tensor([0.0007, 0.0007, 0.0007, 0.0006, 0.0006, 0.0006, 0.1155, 0.1155, 0.1155,\n",
      "        0.0384, 0.0384, 0.0384], device='cuda:2')\n",
      "w_plus: torch.Size([4])\n",
      "w_minus: torch.Size([12])\n",
      "w_plus[:-1]: torch.Size([3])\n",
      "tensor([[10429.9727,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,  8292.7578,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,   883.5525,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,   377.6844]], device='cuda:2')\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [ True,  True,  True, False]])\n",
      "tensor([[1.0430e+04, 1.3678e+04, 1.3808e+04, 1.1907e+04],\n",
      "        [4.7545e+04, 8.2928e+03, 3.9719e+04, 4.7187e+05],\n",
      "        [1.2832e+02, 1.1300e+03, 8.8355e+02, 1.1320e+02],\n",
      "        [8.2587e+03, 4.0467e+02, 1.9934e+02, 3.7768e+02]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    print(\"rate_param:\", rate_param)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device), rate_param.float())\n",
    "    return u_dist.sample()\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool()).to(device)\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool()).to(device)\n",
    "    print(\"s_minus:\", s_minus.size()) #torch.Size([12])\n",
    "    print(\"U:\", U)\n",
    "    w_plus_dist = Gamma(1+a_plus, (U*s_plus + b_plus))\n",
    "    U = U.repeat_interleave(int(len(s_minus)/BS))\n",
    "    print(\"U:\", U)\n",
    "\n",
    "\n",
    "\n",
    "    w_minus_dist = Gamma(a_minus, (U*s_minus + b_minus))\n",
    "\n",
    "\n",
    "\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    print(\"w_plus:\", w_plus.size()) #torch.Size([4])\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    print(\"w_minus:\", w_minus.size()) #torch.Size([4, 12])\n",
    "    print('w_plus[:-1]:', w_plus[:-1].size())\n",
    "\n",
    "    result = torch.zeros(BS, BS).to(device)\n",
    "    diagonal_matrix = torch.diag(w_plus)\n",
    "\n",
    "    # Add the diagonal matrix to the zero matrix to create result\n",
    "    result += diagonal_matrix\n",
    "    print(result)\n",
    "\n",
    "    # Fill b's elements sequentially into the remaining positions of result\n",
    "    mask = ~torch.eye(BS, dtype=bool)  # Create a boolean mask that excludes the diagonal\n",
    "    print(mask)\n",
    "    result[mask] = w_minus\n",
    "\n",
    "    # tmp = torch.cat((w_plus[:-1].unsqueeze(1), w_minus.transpose(1, 0)), dim=1)\n",
    "\n",
    "    \n",
    "    # w_matrix = torch.cat((tmp.view(-1), w_plus[-1].unsqueeze(0))).view(BS, 4)\n",
    "    return result\n",
    "\n",
    "BATCH_SIZE =4\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "for _ in range(2):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [0., 0., 0., 4.]])\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [ True,  True,  True, False]])\n",
      "a's elements on the diagonal:\n",
      "tensor([[ 1.,  5.,  6.,  7.],\n",
      "        [ 8.,  2.,  9., 10.],\n",
      "        [11., 12.,  3., 13.],\n",
      "        [14., 15., 16.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create example tensors a and b\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]).float()\n",
    "\n",
    "# Create a 4x4 zero tensor result\n",
    "result = torch.zeros(4, 4)\n",
    "diagonal_matrix = torch.diag(a)\n",
    "\n",
    "# Add the diagonal matrix to the zero matrix to create result\n",
    "result += diagonal_matrix\n",
    "print(result)\n",
    "\n",
    "# Fill b's elements sequentially into the remaining positions of result\n",
    "b_idx = 0\n",
    "mask = ~torch.eye(4, dtype=bool)  # Create a boolean mask that excludes the diagonal\n",
    "print(mask)\n",
    "result[mask] = b\n",
    "# result = result.int()  # Convert the result to integer type\n",
    "\n",
    "# Output the result\n",
    "print(\"a's elements on the diagonal:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量： tensor([1.0000, 2.5000, 3.7000])\n",
      "重复后的张量： tensor([1.0000, 2.5000, 3.7000, 1.0000, 2.5000, 3.7000, 1.0000, 2.5000, 3.7000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例的浮点数张量\n",
    "a = torch.tensor([1.0, 2.5, 3.7])\n",
    "\n",
    "# 对整个张量重复3次\n",
    "repeated_tensor = a.repeat(3)\n",
    "\n",
    "# 输出结果\n",
    "print(\"原始张量：\", a)\n",
    "print(\"重复后的张量：\", repeated_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
