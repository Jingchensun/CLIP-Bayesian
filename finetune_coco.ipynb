{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6812d0688e7b4b20824d66e7c09c64bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 2; 23.68 GiB total capacity; 15.24 GiB already allocated; 4.00 MiB free; 15.43 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m texts \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(list_txt)\u001b[39m.\u001b[39mto(device) \u001b[39m#torch.Size([32, 77])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m  \u001b[39m# print(texts.size()) #torch.Size([32, 77])\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m image_features, text_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(images),\\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m         model\u001b[39m.\u001b[39mencode_text(texts)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m image_features, text_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39m\\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m         normalize(image_features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m         torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(text_features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=124'>125</a>\u001b[0m logits_per_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(torch\u001b[39m.\u001b[39mmatmul(image_features,\\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m         text_features\u001b[39m.\u001b[39mT))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/clip/model.py:149\u001b[0m, in \u001b[0;36mModifiedResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m x \u001b[39m=\u001b[39m stem(x)\n\u001b[1;32m    148\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 149\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    150\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    151\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/clip/model.py:51\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(out))\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x)\n\u001b[1;32m     53\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m     54\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu3(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:135\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m    134\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    136\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    137\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, bn_training, exponential_average_factor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/nn/functional.py:2149\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2147\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2149\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2150\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2151\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 2; 23.68 GiB total capacity; 15.24 GiB already allocated; 4.00 MiB free; 15.43 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "EPOCH =1\n",
    "BATCH_SIZE =16\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "iters = 2\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"RN50\",device=device,jit=False) #Must set jit=False for training\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device)).\\\n",
    "            reshape(BS, -1)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U.unsqueeze(1)*s_minus + b_minus)\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "            w_minus.transpose(1, 0)), dim=1)\n",
    "    w_matrix = torch.cat((tmp.view(-1),\\\n",
    "            w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    return w_matrix\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    print(\"full_mat:\", full_mat)\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    print(\"rate_param:\", rate_param)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    print(rate_param)\n",
    "    print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco2014/images', text_path='/home/jason/data/coco2014/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco2014/images','/home/jason/data/coco2014/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "        \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "\n",
    "        image_features, text_features = model.encode_image(images),\\\n",
    "                model.encode_text(texts)\n",
    "        image_features, text_features = torch.nn.functional.\\\n",
    "                normalize(image_features, dim=1),\\\n",
    "                torch.nn.functional.normalize(text_features, dim=1)\n",
    "        logits_per_image = torch.exp(torch.matmul(image_features,\\\n",
    "                text_features.T))\n",
    "        print(torch.matmul(image_features,\\\n",
    "                text_features.T))\n",
    "        print(\"logits_per_image :\", logits_per_image )\n",
    "\n",
    "        weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "        for _ in range(iters):\n",
    "            U = sample_u(weights, logits_per_image)\n",
    "            weights = sample_w(U, logits_per_image)\n",
    "\n",
    "        weighted_sim = weights * logits_per_image\n",
    "        total_loss = loss_fn(weighted_sim,\\\n",
    "                torch.arange(BATCH_SIZE).to(device))\n",
    "\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        \n",
    "\n",
    "        # image_features= model.encode_image(images)\n",
    "        # text_features = model.encode_text(texts)\n",
    "        # print(image_features.size(), text_features.size(), \">>>>>>\")\n",
    "\n",
    "        # logits_per_image, logits_per_text = model(images, texts)\n",
    "        # ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        # total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        # total_loss.backward()\n",
    "        # print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_matrix: tensor([[ 19.8787,  24.1028,  10.4320,  82.6808,  36.4419],\n",
      "        [164.7603,   5.4474, 116.0480,   7.0680, 107.5985],\n",
      "        [ 25.7201,  47.8981,  30.3414,  31.4389,   7.6939],\n",
      "        [ 98.1041,  36.6093, 129.1169,  16.4063, 108.7023],\n",
      "        [ 66.2840,  71.9707,  27.8949,  42.4934, 146.5928]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# implementation by Rohan\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device)).\\\n",
    "            reshape(BS, -1)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U.unsqueeze(1)*s_minus + b_minus)\n",
    "    \n",
    "#     print(\"s_minus:\", s_minus.size())\n",
    "#     print(\"w_minus_dist:\", (U.unsqueeze(1)*s_minus).size())\n",
    "    w_plus = w_plus_dist.sample()\n",
    "#     print(\"w_plus:\",w_plus)\n",
    "    w_minus = w_minus_dist.sample()\n",
    "#     print(\"w_minus:\", w_minus)\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "            w_minus.transpose(1, 0)), dim=1)\n",
    "    w_matrix = torch.cat((tmp.view(-1),\\\n",
    "            w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    print(\"w_matrix:\",w_matrix)\n",
    "    return w_matrix\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    # print(rate_param)\n",
    "    # print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "\n",
    "BATCH_SIZE =5\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "for _ in range(1):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    # print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_matrix: tensor([[ 19.8787,  24.1028, 116.0480,  31.4389, 108.7023],\n",
      "        [ 10.4320,   5.4474,   7.0680,   7.6939,  66.2840],\n",
      "        [ 82.6808, 107.5985,  30.3414,  98.1041,  71.9707],\n",
      "        [ 36.4419,  25.7201,  36.6093,  16.4063,  27.8949],\n",
      "        [164.7603,  47.8981, 129.1169,  42.4934, 146.5928]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# implementation by jingchen\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device))\n",
    "    # print(\"s_minus:\", s_minus)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    U = U.repeat_interleave(int(BS-1))\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U*s_minus + b_minus)\n",
    "    \n",
    "    # print(\"s_minus:\", s_minus.size())\n",
    "    # print(\"w_minus_dist:\", w_minus_dist)\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    # print(\"w_plus:\",w_plus)\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    # print(\"w_minus:\", w_minus)\n",
    "    # tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "    #         w_minus.transpose(1, 0)), dim=1)\n",
    "    # w_matrix = torch.cat((tmp.view(-1),\\\n",
    "    #         w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    # print(\"w_matrix:\",w_matrix)\n",
    "\n",
    "\n",
    "    result = torch.zeros(BS, BS).to(device)\n",
    "    diagonal_matrix = torch.diag(w_plus)\n",
    "\n",
    "    # Add the diagonal matrix to the zero matrix to create result\n",
    "    result += diagonal_matrix\n",
    "    # print(result)\n",
    "\n",
    "    # Fill b's elements sequentially into the remaining positions of result\n",
    "    mask = ~torch.eye(BS, dtype=bool)  # Create a boolean mask that excludes the diagonal\n",
    "    # print(mask)\n",
    "    result[mask] = w_minus\n",
    "    print(\"w_matrix:\", result)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    # print(rate_param)\n",
    "    # print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "\n",
    "BATCH_SIZE =5\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "# print(\"similarity_score\", similarity_score)\n",
    "for _ in range(1):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    # print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "# print(weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
