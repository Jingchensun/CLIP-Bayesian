{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6287c69f0aa40579bf2cdf6919bfd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "tensor([[0.3254, 0.1210, 0.0997, 0.1207, 0.0974, 0.1101, 0.1461, 0.0912, 0.0917,\n",
      "         0.0561, 0.1105, 0.0945, 0.1024, 0.1688, 0.1519, 0.0892],\n",
      "        [0.0701, 0.2285, 0.1072, 0.0958, 0.0940, 0.0943, 0.1277, 0.1137, 0.1191,\n",
      "         0.0645, 0.1263, 0.1020, 0.1029, 0.1082, 0.1417, 0.0866],\n",
      "        [0.0550, 0.0947, 0.2563, 0.1093, 0.1048, 0.1561, 0.0985, 0.1228, 0.1550,\n",
      "         0.1448, 0.0695, 0.0952, 0.0883, 0.1293, 0.0864, 0.0508],\n",
      "        [0.0853, 0.0518, 0.0795, 0.2465, 0.1012, 0.1177, 0.0752, 0.1202, 0.1099,\n",
      "         0.0407, 0.0881, 0.0801, 0.1157, 0.0981, 0.1044, 0.0757],\n",
      "        [0.0661, 0.0882, 0.1024, 0.1029, 0.2446, 0.0918, 0.0499, 0.1553, 0.1451,\n",
      "         0.0521, 0.0922, 0.0730, 0.2395, 0.1025, 0.1338, 0.1371],\n",
      "        [0.0953, 0.1569, 0.1696, 0.0985, 0.1222, 0.2500, 0.0837, 0.1227, 0.1567,\n",
      "         0.1401, 0.1188, 0.1174, 0.1144, 0.1760, 0.1415, 0.1294],\n",
      "        [0.0800, 0.1021, 0.0717, 0.0810, 0.1139, 0.0837, 0.2878, 0.0999, 0.1031,\n",
      "         0.0492, 0.0738, 0.0416, 0.1033, 0.0885, 0.1243, 0.0853],\n",
      "        [0.0139, 0.0821, 0.1241, 0.1138, 0.1262, 0.0685, 0.0387, 0.2166, 0.1331,\n",
      "         0.0670, 0.0634, 0.0387, 0.1306, 0.0951, 0.0717, 0.0612],\n",
      "        [0.0331, 0.0706, 0.1624, 0.0827, 0.1179, 0.1375, 0.0834, 0.0946, 0.2434,\n",
      "         0.0829, 0.1039, 0.1002, 0.1239, 0.1192, 0.0792, 0.0969],\n",
      "        [0.0782, 0.1573, 0.1547, 0.1265, 0.0718, 0.1359, 0.1165, 0.1333, 0.1672,\n",
      "         0.3105, 0.1310, 0.1281, 0.1107, 0.1447, 0.1449, 0.0864],\n",
      "        [0.1285, 0.0905, 0.1105, 0.1387, 0.0791, 0.1096, 0.1107, 0.1002, 0.1169,\n",
      "         0.0720, 0.1868, 0.0687, 0.0852, 0.1276, 0.1165, 0.0662],\n",
      "        [0.0854, 0.0906, 0.1439, 0.1384, 0.1166, 0.1682, 0.1227, 0.1018, 0.1403,\n",
      "         0.1816, 0.1097, 0.2242, 0.1136, 0.1483, 0.1025, 0.1044],\n",
      "        [0.0412, 0.0582, 0.0753, 0.1334, 0.2332, 0.0849, 0.0427, 0.1444, 0.1370,\n",
      "         0.0307, 0.0853, 0.0639, 0.2394, 0.0665, 0.1206, 0.1664],\n",
      "        [0.1713, 0.1401, 0.1009, 0.1038, 0.1028, 0.0940, 0.1042, 0.1155, 0.1150,\n",
      "         0.0880, 0.1247, 0.1029, 0.1136, 0.2362, 0.1786, 0.0811],\n",
      "        [0.0141, 0.1024, 0.0677, 0.1111, 0.0698, 0.0814, 0.0516, 0.0878, 0.1063,\n",
      "         0.0620, 0.0931, 0.0519, 0.0693, 0.0534, 0.1750, 0.0964],\n",
      "        [0.0570, 0.0729, 0.1154, 0.1102, 0.1710, 0.1247, 0.1263, 0.1125, 0.1390,\n",
      "         0.0714, 0.1066, 0.0903, 0.1804, 0.1005, 0.1500, 0.2546]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<MmBackward>)\n",
      "logits_per_image : tensor([[1.3848, 1.1289, 1.1045, 1.1279, 1.1025, 1.1162, 1.1572, 1.0957, 1.0957,\n",
      "         1.0576, 1.1172, 1.0996, 1.1074, 1.1836, 1.1641, 1.0938],\n",
      "        [1.0723, 1.2568, 1.1133, 1.1006, 1.0986, 1.0986, 1.1357, 1.1201, 1.1260,\n",
      "         1.0664, 1.1348, 1.1074, 1.1084, 1.1143, 1.1523, 1.0908],\n",
      "        [1.0566, 1.0996, 1.2920, 1.1152, 1.1104, 1.1689, 1.1035, 1.1309, 1.1680,\n",
      "         1.1562, 1.0723, 1.0996, 1.0928, 1.1377, 1.0898, 1.0518],\n",
      "        [1.0889, 1.0527, 1.0830, 1.2793, 1.1064, 1.1250, 1.0781, 1.1279, 1.1162,\n",
      "         1.0420, 1.0918, 1.0830, 1.1230, 1.1035, 1.1104, 1.0791],\n",
      "        [1.0684, 1.0918, 1.1074, 1.1084, 1.2773, 1.0957, 1.0508, 1.1680, 1.1562,\n",
      "         1.0537, 1.0967, 1.0762, 1.2705, 1.1084, 1.1436, 1.1465],\n",
      "        [1.0996, 1.1699, 1.1846, 1.1035, 1.1299, 1.2842, 1.0869, 1.1309, 1.1699,\n",
      "         1.1504, 1.1260, 1.1250, 1.1211, 1.1924, 1.1523, 1.1377],\n",
      "        [1.0830, 1.1074, 1.0742, 1.0840, 1.1211, 1.0869, 1.3340, 1.1055, 1.1084,\n",
      "         1.0508, 1.0762, 1.0430, 1.1084, 1.0928, 1.1318, 1.0889],\n",
      "        [1.0137, 1.0859, 1.1318, 1.1201, 1.1348, 1.0713, 1.0391, 1.2422, 1.1426,\n",
      "         1.0693, 1.0654, 1.0391, 1.1396, 1.0996, 1.0742, 1.0635],\n",
      "        [1.0332, 1.0732, 1.1768, 1.0859, 1.1250, 1.1475, 1.0869, 1.0996, 1.2754,\n",
      "         1.0859, 1.1094, 1.1055, 1.1318, 1.1270, 1.0820, 1.1016],\n",
      "        [1.0811, 1.1699, 1.1670, 1.1348, 1.0742, 1.1455, 1.1240, 1.1426, 1.1816,\n",
      "         1.3643, 1.1396, 1.1367, 1.1172, 1.1553, 1.1562, 1.0898],\n",
      "        [1.1367, 1.0947, 1.1172, 1.1484, 1.0820, 1.1162, 1.1172, 1.1055, 1.1240,\n",
      "         1.0742, 1.2051, 1.0713, 1.0889, 1.1357, 1.1240, 1.0684],\n",
      "        [1.0889, 1.0947, 1.1553, 1.1484, 1.1240, 1.1836, 1.1309, 1.1074, 1.1504,\n",
      "         1.1992, 1.1162, 1.2510, 1.1201, 1.1602, 1.1084, 1.1104],\n",
      "        [1.0420, 1.0596, 1.0781, 1.1426, 1.2627, 1.0889, 1.0439, 1.1553, 1.1465,\n",
      "         1.0312, 1.0889, 1.0664, 1.2705, 1.0684, 1.1279, 1.1807],\n",
      "        [1.1865, 1.1504, 1.1064, 1.1094, 1.1084, 1.0986, 1.1104, 1.1221, 1.1221,\n",
      "         1.0918, 1.1328, 1.1084, 1.1201, 1.2666, 1.1953, 1.0840],\n",
      "        [1.0146, 1.1074, 1.0703, 1.1172, 1.0723, 1.0850, 1.0527, 1.0918, 1.1123,\n",
      "         1.0645, 1.0977, 1.0537, 1.0713, 1.0547, 1.1914, 1.1016],\n",
      "        [1.0586, 1.0752, 1.1221, 1.1162, 1.1865, 1.1328, 1.1348, 1.1191, 1.1494,\n",
      "         1.0742, 1.1123, 1.0947, 1.1973, 1.1055, 1.1621, 1.2900]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<ExpBackward>)\n",
      "full_mat: tensor([[1.3848, 1.1289, 1.1045, 1.1279, 1.1025, 1.1162, 1.1572, 1.0957, 1.0957,\n",
      "         1.0576, 1.1172, 1.0996, 1.1074, 1.1836, 1.1641, 1.0938],\n",
      "        [1.0723, 1.2568, 1.1133, 1.1006, 1.0986, 1.0986, 1.1357, 1.1201, 1.1260,\n",
      "         1.0664, 1.1348, 1.1074, 1.1084, 1.1143, 1.1523, 1.0908],\n",
      "        [1.0566, 1.0996, 1.2920, 1.1152, 1.1104, 1.1689, 1.1035, 1.1309, 1.1680,\n",
      "         1.1562, 1.0723, 1.0996, 1.0928, 1.1377, 1.0898, 1.0518],\n",
      "        [1.0889, 1.0527, 1.0830, 1.2793, 1.1064, 1.1250, 1.0781, 1.1279, 1.1162,\n",
      "         1.0420, 1.0918, 1.0830, 1.1230, 1.1035, 1.1104, 1.0791],\n",
      "        [1.0684, 1.0918, 1.1074, 1.1084, 1.2773, 1.0957, 1.0508, 1.1680, 1.1562,\n",
      "         1.0537, 1.0967, 1.0762, 1.2705, 1.1084, 1.1436, 1.1465],\n",
      "        [1.0996, 1.1699, 1.1846, 1.1035, 1.1299, 1.2842, 1.0869, 1.1309, 1.1699,\n",
      "         1.1504, 1.1260, 1.1250, 1.1211, 1.1924, 1.1523, 1.1377],\n",
      "        [1.0830, 1.1074, 1.0742, 1.0840, 1.1211, 1.0869, 1.3340, 1.1055, 1.1084,\n",
      "         1.0508, 1.0762, 1.0430, 1.1084, 1.0928, 1.1318, 1.0889],\n",
      "        [1.0137, 1.0859, 1.1318, 1.1201, 1.1348, 1.0713, 1.0391, 1.2422, 1.1426,\n",
      "         1.0693, 1.0654, 1.0391, 1.1396, 1.0996, 1.0742, 1.0635],\n",
      "        [1.0332, 1.0732, 1.1768, 1.0859, 1.1250, 1.1475, 1.0869, 1.0996, 1.2754,\n",
      "         1.0859, 1.1094, 1.1055, 1.1318, 1.1270, 1.0820, 1.1016],\n",
      "        [1.0811, 1.1699, 1.1670, 1.1348, 1.0742, 1.1455, 1.1240, 1.1426, 1.1816,\n",
      "         1.3643, 1.1396, 1.1367, 1.1172, 1.1553, 1.1562, 1.0898],\n",
      "        [1.1367, 1.0947, 1.1172, 1.1484, 1.0820, 1.1162, 1.1172, 1.1055, 1.1240,\n",
      "         1.0742, 1.2051, 1.0713, 1.0889, 1.1357, 1.1240, 1.0684],\n",
      "        [1.0889, 1.0947, 1.1553, 1.1484, 1.1240, 1.1836, 1.1309, 1.1074, 1.1504,\n",
      "         1.1992, 1.1162, 1.2510, 1.1201, 1.1602, 1.1084, 1.1104],\n",
      "        [1.0420, 1.0596, 1.0781, 1.1426, 1.2627, 1.0889, 1.0439, 1.1553, 1.1465,\n",
      "         1.0312, 1.0889, 1.0664, 1.2705, 1.0684, 1.1279, 1.1807],\n",
      "        [1.1865, 1.1504, 1.1064, 1.1094, 1.1084, 1.0986, 1.1104, 1.1221, 1.1221,\n",
      "         1.0918, 1.1328, 1.1084, 1.1201, 1.2666, 1.1953, 1.0840],\n",
      "        [1.0146, 1.1074, 1.0703, 1.1172, 1.0723, 1.0850, 1.0527, 1.0918, 1.1123,\n",
      "         1.0645, 1.0977, 1.0537, 1.0713, 1.0547, 1.1914, 1.1016],\n",
      "        [1.0586, 1.0752, 1.1221, 1.1162, 1.1865, 1.1328, 1.1348, 1.1191, 1.1494,\n",
      "         1.0742, 1.1123, 1.0947, 1.1973, 1.1055, 1.1621, 1.2900]],\n",
      "       device='cuda:2', grad_fn=<MulBackward0>)\n",
      "rate_param: tensor([18.1367, 17.8965, 17.9453, 17.6904, 18.0195, 18.3643, 17.6963, 17.5322,\n",
      "        17.8467, 18.3799, 17.8096, 18.2490, 17.8535, 18.1133, 17.3584, 18.1309],\n",
      "       device='cuda:2', grad_fn=<AddBackward0>)\n",
      "tensor([18.1367, 17.8965, 17.9453, 17.6904, 18.0195, 18.3643, 17.6963, 17.5322,\n",
      "        17.8467, 18.3799, 17.8096, 18.2490, 17.8535, 18.1133, 17.3584, 18.1309],\n",
      "       device='cuda:2', grad_fn=<AddBackward0>)\n",
      "tensor([0.0042, 0.1125, 0.0577, 0.0097, 0.0004, 0.0215, 0.0369, 0.0135, 0.0002,\n",
      "        0.0161, 0.0369, 0.1573, 0.0167, 0.0264, 0.0451, 0.0041],\n",
      "       device='cuda:2')\n",
      "w_matrix: tensor([[ 127.9109,  528.7084,  430.5577,  357.5437,  657.9297,  199.2636,\n",
      "          415.8058,  423.0183,  168.2731,  219.4471,  365.7263,  262.7360,\n",
      "          287.7142,  162.8828,  249.6737,  393.2605],\n",
      "        [ 807.4255,   96.7924, 1085.4934, 1599.4636, 1675.6752, 1570.4899,\n",
      "          682.0493, 1466.3026,  905.5458,  629.4855, 1607.1980, 1049.6980,\n",
      "         1142.1946, 1222.3339,  913.6029,  756.4661],\n",
      "        [ 355.2179,  575.9642,  173.7384,  313.2911,  511.4892,  407.8014,\n",
      "          423.7252,  302.6716,  341.4506,  198.3501,  444.4510,  455.4778,\n",
      "          281.8221,  262.5417,  260.9904,  439.4026],\n",
      "        [ 376.1526,  302.0293,  307.1405,  147.8626,  327.9993,  327.8831,\n",
      "          260.3335,  257.5587,  199.0162,  345.2569,  324.6500,  292.4105,\n",
      "          290.7438,  464.9619,  225.6317,  147.8416],\n",
      "        [  91.8193,  172.0095,  127.2415,  132.5897,   50.3361,   79.0630,\n",
      "          147.9334,  120.1831,  143.4499,  102.1384,  138.8954,  130.0907,\n",
      "           97.8733,  154.9699,   94.5895,  119.3307],\n",
      "        [ 966.0425, 1033.8655,  784.5640, 1035.6608,  973.7350,  439.8156,\n",
      "         2113.7441,  705.2224,  627.3100, 1355.7007, 1017.2756, 1834.3195,\n",
      "          924.0724, 1358.1874, 1010.8826,  777.5260],\n",
      "        [4242.1831, 2707.5078, 3274.7734, 3304.4893, 3308.4746, 2535.5247,\n",
      "          952.3498, 2500.5620, 3698.9023, 2786.8093, 4401.9609, 4257.1631,\n",
      "         2681.6970, 3980.3254, 3429.5071, 2976.9207],\n",
      "        [ 305.3675,  138.8141,  116.0387,  208.8667,  211.0734,  278.2076,\n",
      "          301.9146,   93.6894,  123.9609,  229.4818,  306.7227,  178.1545,\n",
      "          251.6839,  277.1611,  157.9987,  118.0517],\n",
      "        [2525.8604, 1350.9808, 1562.4558, 2221.6514, 2587.6016, 1638.0369,\n",
      "         2705.9062, 2517.5210,  678.8506, 1019.0963,  931.5638, 3430.3093,\n",
      "         2283.1414, 2238.2173, 1692.1261, 1939.7065],\n",
      "        [ 344.6272,  381.9987,  231.4027,  393.1469,  367.8852,  253.9899,\n",
      "          336.5461,  505.7532,  318.8395,  151.2478,  187.4192,  337.1312,\n",
      "          346.5190,  319.5753,  133.0393,  365.8596],\n",
      "        [ 181.4386,   58.1538,  136.6640,  169.5447,   65.4330,  148.1744,\n",
      "          212.9259,  112.7897,  165.3472,  108.4845,  152.7556,  186.3023,\n",
      "          161.7635,  235.9276,  194.6946,  332.6986],\n",
      "        [ 104.2598,  137.6156,  113.0408,  129.1771,  133.3937,  102.6055,\n",
      "          169.1029,  104.9784,  132.6631,  113.8010,  148.7038,   85.8138,\n",
      "          217.2395,  112.5579,   90.5271,  107.5041],\n",
      "        [  83.6990,   92.3503,  128.0802,   87.4543,   89.9110,   76.7310,\n",
      "           80.6035,   83.6994,   51.7119,  125.5178,   80.5584,  134.0261,\n",
      "           77.8506,  117.7429,   89.8685,   99.8062],\n",
      "        [ 563.9955,  437.1420,  590.1339,  432.8850,  708.9729,  794.5023,\n",
      "          660.1207,  609.5159,  668.9752,  739.8005,  258.5400,  463.7036,\n",
      "          580.7007,  363.6355,  444.9433,  854.5466],\n",
      "        [ 153.7633,   75.9815,  139.8179,  146.5105,  163.6434,   89.3453,\n",
      "           89.5573,  148.0686,  127.8338,  133.3887,  114.2160,  145.0459,\n",
      "          210.4023,  144.3756,   92.2520,  145.8052],\n",
      "        [  53.3795,   42.1013,   60.0329,   40.2230,   68.2759,   44.4901,\n",
      "           29.2962,   42.9487,   49.7139,   57.6645,   36.9282,   60.8931,\n",
      "           37.7581,   64.1046,   39.6622,   32.8156]], device='cuda:2')\n",
      "full_mat: tensor([[ 177.1266,  596.8622,  475.5476,  403.2841,  725.3932,  222.4202,\n",
      "          481.1815,  463.5025,  184.3774,  232.0910,  408.5849,  288.9070,\n",
      "          318.6210,  192.7871,  290.6357,  430.1287],\n",
      "        [ 865.7746,  121.6522, 1208.4595, 1760.3472, 1840.9518, 1725.3917,\n",
      "          774.6322, 1642.4308, 1019.6233,  671.2872, 1823.7931, 1162.4585,\n",
      "         1266.0067, 1361.9950, 1052.7845,  825.1686],\n",
      "        [ 375.3376,  633.3356,  224.4687,  349.3930,  567.9329,  476.6975,\n",
      "          467.5874,  342.2790,  398.8036,  229.3423,  476.5695,  500.8477,\n",
      "          307.9677,  298.6925,  284.4387,  462.1451],\n",
      "        [ 409.5803,  317.9566,  332.6356,  189.1601,  362.9133,  368.8685,\n",
      "          280.6721,  290.5081,  222.1441,  359.7549,  354.4519,  316.6829,\n",
      "          326.5190,  513.0927,  250.5305,  159.5361],\n",
      "        [  98.0960,  187.7994,  140.9100,  146.9622,   64.2966,   86.6296,\n",
      "          155.4457,  140.3701,  165.8639,  107.6244,  152.3238,  140.0000,\n",
      "          124.3488,  171.7684,  108.1683,  136.8108],\n",
      "        [1062.2694, 1209.5419,  929.3712, 1142.8678, 1100.2064,  564.8022,\n",
      "         2297.4583,  797.5073,  733.9037, 1559.5853, 1145.4285, 2063.6094,\n",
      "         1035.9718, 1619.4792, 1164.8843,  884.5877],\n",
      "        [4594.3174, 2998.3533, 3517.8230, 3582.0146, 3709.1101, 2755.8975,\n",
      "         1270.4198, 2764.2932, 4099.8574, 2928.3269, 4737.2666, 4440.0879,\n",
      "         2972.3887, 4349.5938, 3881.6394, 3241.4712],\n",
      "        [ 309.5424,  150.7434,  131.3367,  233.9552,  239.5188,  298.0407,\n",
      "          313.7081,  116.3798,  141.6351,  245.3932,  326.7915,  185.1137,\n",
      "          286.8312,  304.7690,  169.7252,  125.5452],\n",
      "        [2609.7268, 1449.9297, 1838.6321, 2412.5745, 2911.0518, 1879.5834,\n",
      "         2941.0876, 2768.2898,  865.7997, 1106.6749, 1033.4536, 3792.0999,\n",
      "         2584.1414, 2522.3660, 1830.9333, 2136.7080],\n",
      "        [ 372.5609,  446.9086,  270.0452,  446.1296,  395.1892,  290.9474,\n",
      "          378.2858,  577.8626,  376.7537,  206.3410,  213.5919,  383.2233,\n",
      "          387.1267,  369.1969,  153.8267,  398.7298],\n",
      "        [ 206.2447,   63.6625,  152.6793,  194.7115,   70.8006,  165.3939,\n",
      "          237.8781,  124.6854,  185.8541,  116.5361,  184.0824,  199.5836,\n",
      "          176.1390,  267.9529,  218.8413,  355.4417],\n",
      "        [ 113.5251,  150.6515,  130.5930,  148.3518,  149.9376,  121.4433,\n",
      "          191.2316,  116.2554,  152.6144,  136.4723,  165.9848,  107.3511,\n",
      "          243.3337,  130.5848,  100.3400,  119.3674],\n",
      "        [  87.2137,   97.8516,  138.0864,   99.9234,  113.5303,   83.5499,\n",
      "           84.1456,   96.6957,   59.2868,  129.4402,   87.7175,  142.9263,\n",
      "           98.9098,  125.7918,  101.3653,  117.8376],\n",
      "        [ 669.1939,  502.8841,  652.9508,  480.2318,  785.8245,  872.8663,\n",
      "          732.9661,  683.9197,  750.6372,  807.7119,  292.8773,  513.9684,\n",
      "          650.4529,  460.5813,  531.8463,  926.3151],\n",
      "        [ 156.0157,   84.1436,  149.6488,  163.6797,  175.4691,   96.9362,\n",
      "           94.2801,  161.6608,  142.1902,  141.9860,  125.3700,  152.8365,\n",
      "          225.4017,  152.2711,  109.9096,  160.6136],\n",
      "        [  56.5072,   45.2672,   67.3611,   44.8974,   81.0109,   50.3989,\n",
      "           33.2444,   48.0656,   57.1419,   61.9443,   41.0754,   66.6613,\n",
      "           45.2065,   70.8656,   46.0919,   42.3334]], device='cuda:2',\n",
      "       grad_fn=<MulBackward0>)\n",
      "rate_param: tensor([ 5891.4507, 19122.7578,  6395.8389,  5055.0063,  2127.4180, 19311.4727,\n",
      "        55842.8633,  3579.0293, 34683.0508,  5666.7192,  2920.4871,  2278.0381,\n",
      "         1664.2717, 10315.2285,  2292.4126,   858.0729], device='cuda:2',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([ 5891.4507, 19122.7578,  6395.8389,  5055.0063,  2127.4180, 19311.4727,\n",
      "        55842.8633,  3579.0293, 34683.0508,  5666.7192,  2920.4871,  2278.0381,\n",
      "         1664.2717, 10315.2285,  2292.4126,   858.0729], device='cuda:2',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([3.2498e-04, 1.0284e-05, 4.0117e-06, 1.4996e-04, 4.2790e-04, 2.2009e-06,\n",
      "        2.0171e-05, 6.3720e-04, 7.9415e-05, 2.9624e-04, 1.3651e-04, 4.9166e-04,\n",
      "        2.9503e-05, 3.0413e-05, 3.8619e-05, 5.2166e-03], device='cuda:2')\n",
      "w_matrix: tensor([[  24898.8242,   44509.4727,   64185.6953,   30790.5020,   27003.7285,\n",
      "           65641.1016,   46482.5312,   34964.5898,   77008.3906,   84805.3516,\n",
      "           43838.0938,   35989.0938,   46063.3828,   42111.9180,   44247.1328,\n",
      "           40004.3828],\n",
      "        [ 576764.5625,  320918.8750, 1214405.0000, 1244237.8750,  576816.3750,\n",
      "          893799.2500, 1175347.8750,  738260.6250,  684418.1875,  460470.8438,\n",
      "          372739.2500,  781181.1250, 1137885.5000,  425576.9062,  644947.1250,\n",
      "          941666.2500],\n",
      "        [ 593380.6875,  858725.5625,  496487.2500, 1052809.8750,  566879.3125,\n",
      "         1681567.3750, 1059872.1250,  722955.6250,  917943.9375,  873531.1250,\n",
      "          720935.2500,  987340.7500,  988818.6875,  795110.0000, 1309324.5000,\n",
      "         1076371.2500],\n",
      "        [  22509.2930,   38628.4297,   15112.2969,   10593.4199,   34094.6094,\n",
      "           33498.1836,   33630.7891,   22359.6543,   31777.9902,   54887.1641,\n",
      "           49937.1484,   31047.9023,   24435.8574,   41507.3320,   37221.6055,\n",
      "           18178.2344],\n",
      "        [  65612.8750,   58870.4180,   59986.7383,   76208.2656,   22195.0488,\n",
      "           66594.8203,   44115.3164,   76101.1797,   61883.6992,   27861.8574,\n",
      "           39858.4492,   67619.0938,   21107.4648,   40549.8359,   45714.5039,\n",
      "           57387.2422],\n",
      "        [  97151.6719,  294444.2188,  353701.9375,  449391.4062,  277859.8125,\n",
      "          199624.4062,  449008.0938,  274324.1250,  295750.0938,  255464.3438,\n",
      "          118523.8438,  266648.9062,  204012.5625,  296518.9688,  197216.3750,\n",
      "          168894.3906],\n",
      "        [ 267264.4062,  600275.8750,  558332.9375,  496089.3125,  398395.5312,\n",
      "          236680.0312,  547488.3125,  583935.5625,  424958.4062,  571142.8750,\n",
      "          337730.8750,  653061.8750,  770693.7500,  619687.0000,  536100.5625,\n",
      "          448540.8750],\n",
      "        [  31881.4531,   30430.9336,   28026.1641,   22862.1777,   26709.2207,\n",
      "           23825.3535,   33016.7734,   18254.0000,   18498.1309,   26052.2715,\n",
      "           29172.3184,   16258.7764,   28189.4727,   31480.8262,   20331.0684,\n",
      "           22700.0312],\n",
      "        [ 288979.2812,  301323.7188,  157604.1719,  251020.8906,  298473.4062,\n",
      "          260055.1719,  349096.2188,  240842.0312,  188855.4219,  308200.0000,\n",
      "          265934.3438,  417455.3750,  148931.9844,  162764.1406,  251857.5156,\n",
      "          145526.7188],\n",
      "        [ 473720.3438,  372134.4688,  404480.1875,  316790.2500,  220188.6875,\n",
      "          269814.0000,  438588.3750,  309271.0312,  357057.2500,  340921.0938,\n",
      "          342653.8438,  280595.8750,  238473.4688,  427246.0312,  261193.0156,\n",
      "          386878.2500],\n",
      "        [  32822.8477,   76350.8594,   20307.7695,   30270.5586,   66419.9844,\n",
      "           54716.4258,   27141.2441,   40361.8438,   33549.1484,   20435.1035,\n",
      "           25699.8008,   35320.5273,   60324.0078,   20587.4551,   38686.0977,\n",
      "           32188.5918],\n",
      "        [ 119665.0938,  104632.3203,   71205.2656,   64882.1719,   60910.9180,\n",
      "           87276.5859,  109827.5234,   68803.4922,   72873.5234,   90387.4141,\n",
      "           98040.3438,   37005.0078,  111069.6953,   83024.7188,   71278.3125,\n",
      "           61574.9844],\n",
      "        [  66408.8828,   70226.3984,   92676.1719,  101292.5938,   41296.2852,\n",
      "           54782.2812,   44941.1875,   58960.6953,   53726.4609,  169608.1094,\n",
      "           98147.2500,   94342.0938,   53020.0547,   59926.2969,   50992.9102,\n",
      "           67592.1875],\n",
      "        [  81937.0000,  120705.2188,  126206.7734,   81839.4766,   64932.5156,\n",
      "          134673.9531,  121776.0156,   76534.1250,  142259.4688,  128102.5234,\n",
      "          118622.9375,   79321.7188,  174776.6562,   63764.9453,  101198.9219,\n",
      "           94215.7109],\n",
      "        [ 224807.0469,  214162.1250,  260821.9062,  126095.0859,  186541.5156,\n",
      "          122293.1719,  169061.7031,  142459.7344,  204010.4688,  113377.8438,\n",
      "          143328.7500,  195696.6406,  208863.0938,  154011.7656,   46181.6211,\n",
      "          215837.9062],\n",
      "        [  11858.1318,   14607.5869,    6455.2583,    8170.4131,   15350.5918,\n",
      "           12588.4355,   21265.9082,    9282.3838,    8240.2881,   15266.3926,\n",
      "           12719.5820,   14164.6367,   13900.6299,   13566.5186,   13256.6367,\n",
      "            4150.3096]], device='cuda:2')\n",
      "total loss: tensor(230921.9844, device='cuda:2', grad_fn=<NllLossBackward>)\n",
      "16\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<MmBackward>)\n",
      "logits_per_image : tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<ExpBackward>)\n",
      "full_mat: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:2', grad_fn=<MulBackward0>)\n",
      "rate_param: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:2', grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The parameter rate has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(BATCH_SIZE, BATCH_SIZE)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iters):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m     U \u001b[39m=\u001b[39m sample_u(weights, logits_per_image)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m     weights \u001b[39m=\u001b[39m sample_w(U, logits_per_image)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m weighted_sim \u001b[39m=\u001b[39m weights \u001b[39m*\u001b[39m logits_per_image\n",
      "\u001b[1;32m/home/jason/CLIP-Bayesian/finetune_coco.ipynb 单元格 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m rate_param \u001b[39m=\u001b[39m b_u \u001b[39m+\u001b[39m full_mat\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrate_param:\u001b[39m\u001b[39m\"\u001b[39m, rate_param)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m u_dist \u001b[39m=\u001b[39m Gamma(torch\u001b[39m.\u001b[39;49mtensor(a_u)\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(device),\\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m         rate_param\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mprint\u001b[39m(rate_param)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224135303030227d/home/jason/CLIP-Bayesian/finetune_coco.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(u_dist\u001b[39m.\u001b[39msample())\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/distributions/gamma.py:48\u001b[0m, in \u001b[0;36mGamma.__init__\u001b[0;34m(self, concentration, rate, validate_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcentration\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 48\u001b[0m \u001b[39msuper\u001b[39;49m(Gamma, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch182/lib/python3.8/site-packages/torch/distributions/distribution.py:53\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m constraint\u001b[39m.\u001b[39mcheck(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param))\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 53\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has invalid values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(param))\n\u001b[1;32m     54\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter rate has invalid values"
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "EPOCH =1\n",
    "BATCH_SIZE =16\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "iters = 2\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"RN50\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device))\n",
    "    # print(\"s_minus:\", s_minus)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    U = U.repeat_interleave(int(BS-1))\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U*s_minus + b_minus)\n",
    "    \n",
    "    # print(\"s_minus:\", s_minus.size())\n",
    "    # print(\"w_minus_dist:\", w_minus_dist)\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    # print(\"w_plus:\",w_plus)\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    # print(\"w_minus:\", w_minus)\n",
    "    # tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "    #         w_minus.transpose(1, 0)), dim=1)\n",
    "    # w_matrix = torch.cat((tmp.view(-1),\\\n",
    "    #         w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    # print(\"w_matrix:\",w_matrix)\n",
    "\n",
    "\n",
    "    result = torch.zeros(BS, BS).to(device)\n",
    "    diagonal_matrix = torch.diag(w_plus)\n",
    "\n",
    "    # Add the diagonal matrix to the zero matrix to create result\n",
    "    result += diagonal_matrix\n",
    "    # print(result)\n",
    "\n",
    "    # Fill b's elements sequentially into the remaining positions of result\n",
    "    mask = ~torch.eye(BS, dtype=bool)  # Create a boolean mask that excludes the diagonal\n",
    "    # print(mask)\n",
    "    result[mask] = w_minus\n",
    "    print(\"w_matrix:\", result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    print(\"full_mat:\", full_mat)\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    print(\"rate_param:\", rate_param)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    print(rate_param)\n",
    "    print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "class cocodtrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path='/home/jason/data/coco2014/images', text_path='/home/jason/data/coco2014/text', mode='train2014'):\n",
    "\n",
    "        self.image_list = []\n",
    "        self.image_list.extend(glob.glob(os.path.join(image_path, mode, '*.jpg')))\n",
    "        self.image_list.sort()\n",
    "\n",
    "        self.label_list = []\n",
    "        self.label_list.extend(glob.glob(os.path.join(text_path, mode, '*.txt')))\n",
    "        self.label_list.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index]).convert(\"RGB\")\n",
    "        image = image.resize((224,224), Image.BILINEAR)\n",
    "        image = preprocess(image)\n",
    "        #image = np.asarray(image)\n",
    "\n",
    "        with open(self.label_list[index], \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            label = random.choice(data)\n",
    "            \n",
    "        return image, label\n",
    "trainset = cocodtrain('/home/jason/data/coco2014/images','/home/jason/data/coco2014/text','train2014')\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=16,\n",
    "                    drop_last=True)\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        # p.grad.data = p.grad.data.float() \n",
    "\n",
    "#device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "#model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "#clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch:', epoch)\n",
    "    for batch in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "        # print(list_image.size()) #torch.Size([32, 3, 224, 224])\n",
    "        print(len(list_txt))\n",
    "        \n",
    "        images = torch.tensor(np.stack(list_image)).to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device) #torch.Size([32, 77])\n",
    "         # print(texts.size()) #torch.Size([32, 77])\n",
    "\n",
    "        image_features, text_features = model.encode_image(images),\\\n",
    "                model.encode_text(texts)\n",
    "        image_features, text_features = torch.nn.functional.\\\n",
    "                normalize(image_features, dim=1),\\\n",
    "                torch.nn.functional.normalize(text_features, dim=1)\n",
    "        logits_per_image = torch.exp(torch.matmul(image_features,\\\n",
    "                text_features.T))\n",
    "        print(torch.matmul(image_features,\\\n",
    "                text_features.T))\n",
    "        print(\"logits_per_image :\", logits_per_image )\n",
    "\n",
    "        weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "        for _ in range(iters):\n",
    "            U = sample_u(weights, logits_per_image)\n",
    "            weights = sample_w(U, logits_per_image)\n",
    "\n",
    "        weighted_sim = weights * logits_per_image\n",
    "        total_loss = loss_fn(weighted_sim,\\\n",
    "                torch.arange(BATCH_SIZE).to(device))\n",
    "\n",
    "        total_loss.backward()\n",
    "        print('total loss:', total_loss)\n",
    "      \n",
    "        \n",
    "\n",
    "        # image_features= model.encode_image(images)\n",
    "        # text_features = model.encode_text(texts)\n",
    "        # print(image_features.size(), text_features.size(), \">>>>>>\")\n",
    "\n",
    "        # logits_per_image, logits_per_text = model(images, texts)\n",
    "        # ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
    "\n",
    "        # total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        # total_loss.backward()\n",
    "        # print('total loss:', total_loss)\n",
    "      \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "    \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_matrix: tensor([[ 19.8787,  24.1028,  10.4320,  82.6808,  36.4419],\n",
      "        [164.7603,   5.4474, 116.0480,   7.0680, 107.5985],\n",
      "        [ 25.7201,  47.8981,  30.3414,  31.4389,   7.6939],\n",
      "        [ 98.1041,  36.6093, 129.1169,  16.4063, 108.7023],\n",
      "        [ 66.2840,  71.9707,  27.8949,  42.4934, 146.5928]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# implementation by Rohan\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device)).\\\n",
    "            reshape(BS, -1)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U.unsqueeze(1)*s_minus + b_minus)\n",
    "    \n",
    "#     print(\"s_minus:\", s_minus.size())\n",
    "#     print(\"w_minus_dist:\", (U.unsqueeze(1)*s_minus).size())\n",
    "    w_plus = w_plus_dist.sample()\n",
    "#     print(\"w_plus:\",w_plus)\n",
    "    w_minus = w_minus_dist.sample()\n",
    "#     print(\"w_minus:\", w_minus)\n",
    "    tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "            w_minus.transpose(1, 0)), dim=1)\n",
    "    w_matrix = torch.cat((tmp.view(-1),\\\n",
    "            w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    print(\"w_matrix:\",w_matrix)\n",
    "    return w_matrix\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    # print(rate_param)\n",
    "    # print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "\n",
    "BATCH_SIZE =5\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "for _ in range(1):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    # print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_matrix: tensor([[ 19.8787,  24.1028, 116.0480,  31.4389, 108.7023],\n",
      "        [ 10.4320,   5.4474,   7.0680,   7.6939,  66.2840],\n",
      "        [ 82.6808, 107.5985,  30.3414,  98.1041,  71.9707],\n",
      "        [ 36.4419,  25.7201,  36.6093,  16.4063,  27.8949],\n",
      "        [164.7603,  47.8981, 129.1169,  42.4934, 146.5928]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# implementation by jingchen\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions.gamma import Gamma\n",
    "device = \"cuda:2\"\n",
    "a_u = 1\n",
    "b_u = 0\n",
    "\n",
    "a_minus = 10\n",
    "b_minus = 0\n",
    "\n",
    "a_plus = 5\n",
    "b_plus = 0\n",
    "\n",
    "def sample_w(U, s_matrix):\n",
    "    BS = s_matrix.shape[0]\n",
    "    s_plus = s_matrix.masked_select(torch.eye(BS).bool().to(device))\n",
    "    s_minus = s_matrix.masked_select(~torch.eye(BS).bool().to(device))\n",
    "    # print(\"s_minus:\", s_minus)\n",
    "    w_plus_dist = Gamma(torch.tensor(1+a_plus).float().to(device),\\\n",
    "            U*s_plus + b_plus)\n",
    "    U = U.repeat_interleave(int(BS-1))\n",
    "    w_minus_dist = Gamma(torch.tensor(a_minus).float().to(device),\\\n",
    "            U*s_minus + b_minus)\n",
    "    \n",
    "    # print(\"s_minus:\", s_minus.size())\n",
    "    # print(\"w_minus_dist:\", w_minus_dist)\n",
    "    w_plus = w_plus_dist.sample()\n",
    "    # print(\"w_plus:\",w_plus)\n",
    "    w_minus = w_minus_dist.sample()\n",
    "    # print(\"w_minus:\", w_minus)\n",
    "    # tmp = torch.cat((w_plus[:-1].unsqueeze(1),\\\n",
    "    #         w_minus.transpose(1, 0)), dim=1)\n",
    "    # w_matrix = torch.cat((tmp.view(-1),\\\n",
    "    #         w_plus[-1].unsqueeze(0))).view(BS, BS)\n",
    "    # print(\"w_matrix:\",w_matrix)\n",
    "\n",
    "\n",
    "    result = torch.zeros(BS, BS).to(device)\n",
    "    diagonal_matrix = torch.diag(w_plus)\n",
    "\n",
    "    # Add the diagonal matrix to the zero matrix to create result\n",
    "    result += diagonal_matrix\n",
    "    # print(result)\n",
    "\n",
    "    # Fill b's elements sequentially into the remaining positions of result\n",
    "    mask = ~torch.eye(BS, dtype=bool)  # Create a boolean mask that excludes the diagonal\n",
    "    # print(mask)\n",
    "    result[mask] = w_minus\n",
    "    print(\"w_matrix:\", result)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "def sample_u(w_matrix, sim_matrix):\n",
    "    full_mat = w_matrix * sim_matrix\n",
    "    rate_param = b_u + full_mat.sum(dim=1)\n",
    "    u_dist = Gamma(torch.tensor(a_u).float().to(device),\\\n",
    "            rate_param.float())\n",
    "    # print(rate_param)\n",
    "    # print(u_dist.sample())\n",
    "    return u_dist.sample()\n",
    "\n",
    "BATCH_SIZE =5\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "weights = torch.ones(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "\n",
    "similarity_score = torch.rand(BATCH_SIZE, BATCH_SIZE).to(device)\n",
    "# print(\"similarity_score\", similarity_score)\n",
    "for _ in range(1):\n",
    "    U = sample_u(weights, similarity_score)\n",
    "    # print(\"U:\", U.size())\n",
    "    weights = sample_w(U, similarity_score)\n",
    "# print(weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73bc775c6f94d98a067ce096eff928d580e9c541aafc395dafbb8814a34bdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch171': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
